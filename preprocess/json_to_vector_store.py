# ============================================================
# Convert structured JSON (content.json) ‚Üí Vector Database (Chroma)
# with multilingual embedding model: BAAI/bge-m3
# ============================================================

# üîß C√†i ƒë·∫∑t th∆∞ vi·ªán (ch·∫°y 1 l·∫ßn trong terminal)
# pip install llama-index
# pip install llama-index-embeddings-huggingface
# pip install llama-index-vector-stores-chroma
# pip install chromadb
# pip install sentence-transformers

import json
from pathlib import Path
from llama_index.core import (
    Document,
    VectorStoreIndex,
    StorageContext,
    load_index_from_storage,
)
from llama_index.llms.gemini import Gemini
from llama_index.core import Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
from chromadb import Client


# ============================================================
# 1Ô∏è‚É£ ƒê·ªçc d·ªØ li·ªáu JSON
# ============================================================

DATA_PATH = Path("content.json")

with open(DATA_PATH, "r", encoding="utf-8") as f:
    data = json.load(f)

# ============================================================
# 2Ô∏è‚É£ Chuy·ªÉn JSON l·ªìng nhau ‚Üí danh s√°ch Document
# ============================================================

def flatten_json_to_documents(data):
    documents = []

    def process_section(section_name, section_content):
        if isinstance(section_content, list):
            for item in section_content:
                text_parts = []
                for key, value in item.items():
                    if isinstance(value, list):
                        joined = "; ".join(value)
                        text_parts.append(f"{key}: {joined}")
                    else:
                        text_parts.append(f"{key}: {value}")
                text = f"Ph·∫ßn: {section_name}\n" + "\n".join(text_parts)
                documents.append(Document(text=text, metadata={"section": section_name}))

        elif isinstance(section_content, dict):
            text_parts = []
            for k, v in section_content.items():
                if isinstance(v, list):
                    joined = "; ".join(v)
                    text_parts.append(f"{k}: {joined}")
            text = f"Ph·∫ßn: {section_name}\n" + "\n".join(text_parts)
            documents.append(Document(text=text, metadata={"section": section_name}))

    for key, value in data.items():
        process_section(key, value)

    return documents


documents = flatten_json_to_documents(data)
print(f"‚úÖ T·∫°o {len(documents)} documents t·ª´ file JSON.")

# ============================================================
# 3Ô∏è‚É£ T·∫°o Embedding Model (BAAI/bge-m3)
# ============================================================

embed_model = HuggingFaceEmbedding(model_name="intfloat/multilingual-e5-base", 
    cache_folder="./models")
Settings.embed_model = embed_model

# T·∫°o service context cho index
# service_context = ServiceContext.from_defaults()

# ============================================================
# 4Ô∏è‚É£ T·∫°o Vector Store (Chroma)
# ============================================================

client = Client()
collection = client.create_collection("scam_detection_bge")
vector_store = ChromaVectorStore(chroma_collection=collection)

# ============================================================
# 5Ô∏è‚É£ T·∫°o Index t·ª´ Documents
# ============================================================

index = VectorStoreIndex.from_documents(
    documents,
    vector_store=vector_store,
)

index.storage_context.persist("./storage_bge")

print("‚úÖ Index ƒë√£ ƒë∆∞·ª£c t·∫°o v·ªõi BAAI/bge-m3 v√† l∆∞u th√†nh c√¥ng!")

# D√πng Gemini 1.5 Flash

# ============================================================
# 6Ô∏è‚É£ Truy v·∫•n th·ª≠ nghi·ªám
# ============================================================

# query_engine = index.as_query_engine()

# queries = [
#     "Nh·ªØng d·∫•u hi·ªáu nh·∫≠n bi·∫øt t√†i kho·∫£n gi·∫£ m·∫°o l√† g√¨?",
#     "V√≠ d·ª• v·ªÅ tin nh·∫Øn l·ª´a ƒë·∫£o tuy·ªÉn d·ª•ng?",
#     "C√°c ƒë·∫∑c ƒëi·ªÉm c·ªßa b√†i ƒëƒÉng ƒë·∫ßu t∆∞ ·∫£o?",
# ]

# for q in queries:
#     print(f"\nüß† C√¢u h·ªèi: {q}")
#     response = query_engine.query(q)
#     print("üí¨ Tr·∫£ l·ªùi:", response)

# ============================================================
# 7Ô∏è‚É£ (Tu·ª≥ ch·ªçn) T·∫£i l·∫°i index ƒë√£ l∆∞u
# ============================================================
# storage_context = StorageContext.from_defaults(persist_dir="./storage_bge")
# index = load_index_from_storage(storage_context, service_context=service_context)
# query_engine = index.as_query_engine()
# print(query_engine.query("D·∫•u hi·ªáu nghi ng·ªù khi xem avatar l√† g√¨?"))
